#pragma kernel ScaleBias     
#pragma kernel ScaleBias_CNyx
#pragma kernel ScaleBias_CNyx2
#pragma kernel ScaleBias_Flat
#pragma kernel ScaleBias_Loop
#pragma kernel Upsample2D
#pragma kernel AvgPool2D
#pragma kernel MaxPool2D
#pragma kernel AvgPool2D_NoPads
#pragma kernel MaxPool2D_NoPads
//#pragma kernel MaxPool2D_Pool2x2_NoPads
#pragma kernel GlobalAvgPool2D
#pragma kernel GlobalAvgVariancePool2D
#pragma kernel InstanceNorm
#pragma kernel InstanceNormTail_CNyx2
#pragma kernel InstanceNormTail_Flat
#pragma kernel InstanceNormTail_Loop
#pragma kernel Copy

/*
ScaleBias_Flat+ScaleBias_CNyx2 (NEW) vs ScaleBias+ScaleBias_CNyx
Compute Precompiled

MOBILENET@4
<<<Exec #64:  66.5 ms, cpu: 7.7 ms, avg:  66.3 ms, result:OK    <--- NEW!
<<<Exec #64:  66.7 ms, cpu: 8.0 ms, avg:  67.1 ms, result:OK
*/

#include "Tensor.cginc"

TENSOR_DECL(X)
TENSOR_DECL(W)
TENSOR_DECL(B)
TENSOR_DECL(WBK)
TENSOR_DECL_RW(O)

uint4 _Pool;
uint4 _Stride;
uint4 _Pad;
float _Alpha;
float _Epsilon;
uint _LoopStride;

NUMTHREADS((4,8,8), (4,8,4), (4,4,4))
void ScaleBias(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_SHARED2_ARGS4(X, W, B, WBK, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    float bias = B.Get(0, 0, 0, c);
    float scale = W.Get(0, 0, 0, c);

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = v * scale + bias;
        O.Set(n, y, x, c, v);
    }
}

NUMTHREADS((16,16,1), (16,8,1), (16,4,1))
void ScaleBias_CNyx(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.batch * O.height * O.width, 1);
    TENSOR_SHARED2_ARGS4(X, W, B, WBK, O);

    uint c = dispatchThreadID.x;
    uint nyx = dispatchThreadID.y;

    uint x = nyx % X.width;
    uint ny = nyx / X.width;
    uint y = ny % X.height;
    uint n = ny / X.height;

    if (c >= X.channels) return;
    if (n >= X.batch) return;

    float bias = B.Get(0, 0, 0, c);
    float scale = W.Get(0, 0, 0, c);

    float v = X.Get(n, y, x, c);
    v = v * scale + bias;
    O.Set(n, y, x, c, v);
}

NUMTHREADS((256,1,1), (128,1,1), (64,1,1))
void ScaleBias_Flat(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.length, 1, 1);
    TENSOR_SHARED2_ARGS4(X, W, B, WBK, O);

    uint i = dispatchThreadID.x;
    if (i > O.GetLength()) return;

    uint c = i % X.channels;
    float bias = B.Get(c);
    float scale = W.Get(c);

    float v = X.Get(i);
    v = v * scale + bias;
    O.Set(i, v);
}

NUMTHREADS((256,1,1), (128,1,1), (64,1,1))
void ScaleBias_Loop(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.length, 1, 1);
    TENSOR_SHARED2_ARGS4(X, W, B, WBK, O);

    uint i = dispatchThreadID.x;
    uint len = O.GetLength();

    while (i < len)
    {
        uint c = i % X.channels;
        float bias = B.Get(c);
        float scale = W.Get(c);
        
        float v = X.Get(i);
        v = v * scale + bias;
        O.Set(i, v);
    
        i += _LoopStride;
    }
}

NUMTHREADS((32,4,1), (32,2,1), (16,2,1))
void ScaleBias_CNyx2(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.batch * O.height * O.width, 1);
    TENSOR_SHARED2_ARGS4(X, W, B, WBK, O);

    uint c = dispatchThreadID.x;
    uint i = dispatchThreadID.y * X.channels + c;

    if (c >= X.channels) return;
    if (i >= X.GetLength()) return;

    float bias = B.Get(c);
    float scale = W.Get(c);

    float v = X.Get(i);
    v = v * scale + bias;
    O.Set(i, v);
}

NUMTHREADS((4,8,8), (4,8,4), (4,4,4))
void Upsample2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    // NOTE: dispatched over X (not O)
    DISPATCH_ARGS(X.channels, X.width, X.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= X.channels) return;
    if (x >= X.width) return;
    if (y >= X.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        float v = X.Get(n, y, x, c);

        for (uint dy = 0; dy < _Pool.y; ++dy)
            for (uint dx = 0; dx < _Pool.x; ++dx)
            {
                uint oy = y * _Pool.y + dy;
                uint ox = x * _Pool.x + dx;
                O.Set(n, oy, ox, c, v);
            }
    }
}

NUMTHREADS((4,8,8), (4,8,4), (4,4,4))
void MaxPool2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float maxV = -FLT_MAX;
        for (uint dy = 0; dy < _Pool.y; ++dy)
            for (uint dx = 0; dx < _Pool.x; ++dx)
            {
                uint oy = y * _Stride.y + dy;
                uint ox = x * _Stride.x + dx;

                bool mask = (oy >= _Pad.y) && (ox >= _Pad.x) && (oy - _Pad.y < X.height) && (ox - _Pad.x < X.width);
                float v = (mask)? X.Get(n, oy - _Pad.y, ox - _Pad.x, c): -FLT_MAX;
                
                maxV = max(v, maxV);
            }
        
        O.Set(n, y, x, c, maxV);
    }
}

NUMTHREADS((4,8,8), (4,8,4), (4,4,4))
void AvgPool2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float acc = 0;
        float counter = 0;
        for (uint dy = 0; dy < _Pool.y; ++dy)
            for (uint dx = 0; dx < _Pool.x; ++dx)
            {
                uint oy = y * _Stride.y + dy;
                uint ox = x * _Stride.x + dx;

                bool mask = (oy >= _Pad.y) && (ox >= _Pad.x) && (oy - _Pad.y < X.height) && (ox - _Pad.x < X.width);
                acc += (mask)? X.Get(n, oy - _Pad.y, ox - _Pad.x, c): 0;
                counter += (mask)? 1: 0;
            }
        
        acc /= counter;
        O.Set(n, y, x, c, acc);
    }
}

NUMTHREADS((4,8,8), (4,8,4), (4,4,4))
void MaxPool2D_NoPads(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float maxV = -FLT_MAX;
        for (uint dy = 0; dy < _Pool[1]; ++dy)
            for (uint dx = 0; dx < _Pool[0]; ++dx)
            {
                float v = X.Get(n, y * _Stride[1] + dy, x * _Stride[0] + dx, c);
                maxV = max(v, maxV);
            }
        
        O.Set(n, y, x, c, maxV);
    }
}

NUMTHREADS((4,8,8), (4,8,4), (4,4,4))
void AvgPool2D_NoPads(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    float invPoolSize = 1.0f / (_Pool[0] * _Pool[1]);
    for (uint n = 0; n < X.batch; ++n)
    {
        float v = 0;
        for (uint dy = 0; dy < _Pool[1]; ++dy)
            for (uint dx = 0; dx < _Pool[0]; ++dx)
                v += X.Get(n, y * _Stride[1] + dy, x * _Stride[0] + dx, c) * invPoolSize;

        O.Set(n, y, x, c, v);
    }
}

NUMTHREADS((4,8,8), (4,8,4), (4,4,4))
//NUMTHREADS((16,4,4), (16,4,2), (16,2,2))
void MaxPool2D_Pool2x2_NoPads(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v0 = X.Get(n, y*2,   x*2,   c);
        float v1 = X.Get(n, y*2+1, x*2,   c);
        float v2 = X.Get(n, y*2,   x*2+1, c);
        float v3 = X.Get(n, y*2+1, x*2+1, c);
        float v = max(v0, max(v1, max(v2, v3)));

        O.Set(n, y, x, c, v);
    }
}

[numthreads(32,1,1)]
void GlobalAvgPool2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, 1, 1);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    if (c >= O.channels) return;
    //ASSERT(X.batch == O.batch)

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = 0;
        for (uint y = 0; y < X.height; ++y)
            for (uint x = 0; x < X.width; ++x)
                v += X.Get(n, y, x, c);
        
        v /= (X.height * X.width);
        O.Set(n, 0, 0, c, v);
    }
}



#undef GROUP_SIZE
#define GROUP_SIZE BARRACUDA_MAX_THREAD_COUNT

groupshared float AvgVariancePool2D_SharedMean[1][GROUP_SIZE];
groupshared float AvgVariancePool2D_SharedVariance[1][GROUP_SIZE];

[numthreads(1, GROUP_SIZE, 1)]
void GlobalAvgVariancePool2D(uint3 dispatchThreadID : SV_DispatchThreadID, uint3 groupThreadID : SV_GroupThreadID)
{
    DISPATCH_ARGS(O.channels, 1, 1);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;

    for (uint n = 0; n < X.batch; ++n)
    {
        uint tid = groupThreadID.y;
        uint q = (X.height * X.width) / GROUP_SIZE;
        uint r = (X.height * X.width) % GROUP_SIZE;

        float mean = 0;
        float mean2 = 0;

        for (int j = 0; j < q; j++)
        {
            float v = X.Get(n, tid + GROUP_SIZE * j, c);

            // https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf
            // Squential addressing with none divergent branching
            AvgVariancePool2D_SharedMean[groupThreadID.x][tid] = v;
            AvgVariancePool2D_SharedVariance[groupThreadID.x][tid] = v * v;

            GroupMemoryBarrierWithGroupSync();

            for (int s = GROUP_SIZE / 2; s > 0; s >>= 1)
            {
                if (tid < s)
                {
                    AvgVariancePool2D_SharedMean[groupThreadID.x][tid] += AvgVariancePool2D_SharedMean[groupThreadID.x][tid + s];
                    AvgVariancePool2D_SharedVariance[groupThreadID.x][tid] += AvgVariancePool2D_SharedVariance[groupThreadID.x][tid + s];
                }
                GroupMemoryBarrierWithGroupSync();
            }

            mean = mean + AvgVariancePool2D_SharedMean[groupThreadID.x][0];
            mean2 = mean2 + AvgVariancePool2D_SharedVariance[groupThreadID.x][0];
        }


        // N.B: you can reduce this part, but the extra logic wasn't worth it perf wise
        if (tid == 0)
        {
            for (int j = 0; j < r; j++)
            {
                float v = X.Get(n, q * GROUP_SIZE + j, c);
                mean += v;
                mean2 += v * v;
            }
            mean /= (X.height * X.width);
            mean2 /= (X.height * X.width);

            O.Set(n, 0, 0, c, mean);
            O.Set(n, 1, 0, c, mean2 - mean * mean);
        }

    }
}



[numthreads(64,1,1)]
void InstanceNorm(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, 1, 1);
    TENSOR_SHARED2_ARGS4(X, W, B, WBK, O);

    uint c = dispatchThreadID.x;
    if (c >= O.channels) return;
    //ASSERT(X.shape == O.shape)

    float gamma = W.Get(0, 0, 0, c);
    float beta = B.Get(0, 0, 0, c);

    for (uint n = 0; n < O.batch; ++n)
    {
        uint x, y;
        // calc mean/variance
        float mean = 0.0, mean2 = 0.0;
        for (y = 0; y < O.height; ++y)
        {
            for (x = 0; x < O.width; ++x)
            {
                float v = X.Get(n, y, x, c);
                mean += v;
                mean2 += v * v;
            }
        }
        mean /= (O.width * O.height);
        mean2 /= (O.width * O.height);

        float var = mean2 - mean * mean;

        // normalization factor
        float invNormFactor = 1 / sqrt(var + _Epsilon);

        float scale = gamma * invNormFactor;
        float bias = beta - gamma * mean * invNormFactor;

        // apply normalization
        for (y = 0; y < O.height; ++y)
        {
            for (x = 0; x < O.width; ++x)
            {
                float v = X.Get(n, y, x, c);
                v = v * scale + bias;
                O.Set(n, y, x, c, v);
            }
        }
    }
}

NUMTHREADS((256,1,1), (128,1,1), (64,1,1))
void InstanceNormTail_Flat(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.length, 1, 1);
    TENSOR_ARGS3(X, W, O);

    uint i = dispatchThreadID.x;
    if (i > O.GetLength()) return;

    uint c = i % X.channels;
    uint b = i / (X.height * X.width * X.channels);

    float mean = W.Get(b, 0, 0, c);
    float variance = W.Get(b, 1, 0, c);

    // normalization factor
    float invNormFactor = 1 / sqrt(variance + _Epsilon);

    float v = X.Get(i);
    //v = gamma * (v * invNormFactor - mean * invNormFactor) + beta
    v = v * invNormFactor - mean * invNormFactor;

    O.Set(i, v);
}

NUMTHREADS((256,1,1), (128,1,1), (64,1,1))
void InstanceNormTail_Loop(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.length, 1, 1);
    TENSOR_ARGS3(X, W, O);

    uint i = dispatchThreadID.x;
    uint len = O.GetLength();

    while (i < len)
    {
        uint c = i % X.channels;
        uint b = i / (X.height * X.width * X.channels);

        float mean = W.Get(b, 0, 0, c);
        float variance = W.Get(b, 1, 0, c);

        // normalization factor
        float invNormFactor = 1 / sqrt(variance + _Epsilon);

        float v = X.Get(i);
        //v = gamma * (v * invNormFactor - mean * invNormFactor) + beta
        v = v * invNormFactor - mean * invNormFactor;

        O.Set(i, v);

        i += _LoopStride;
    }
}

NUMTHREADS((32,4,1), (32,2,1), (16,2,1))
void InstanceNormTail_CNyx2(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.batch * O.height * O.width, 1);
    TENSOR_ARGS3(X, W, O);

    uint c = dispatchThreadID.x;
    uint i = dispatchThreadID.y * X.channels + c;
    uint b = i / (X.height * X.width * X.channels);

    if (c >= X.channels) return;
    if (i >= X.GetLength()) return;

    float mean = W.Get(b, 0, 0, c);
    float variance = W.Get(b, 1, 0, c);

    // normalization factor
    float invNormFactor = 1 / sqrt(variance + _Epsilon);

    float v = X.Get(i);
    //v = gamma * (v * invNormFactor - mean * invNormFactor) + beta
    v = v * invNormFactor - mean * invNormFactor;

    O.Set(i, v);
}

NUMTHREADS((4,8,8), (4,8,4), (4,4,4))
void Copy(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    // NOTE: dispatched over X (not O)
    DISPATCH_ARGS(X.channels, X.width, X.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= X.channels) return;    if (x >= X.width) return;       if (y >= X.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        O.Set(n + _Pad[0], y + _Pad[1], x + _Pad[2], c + _Pad[3], v);
    }
}
